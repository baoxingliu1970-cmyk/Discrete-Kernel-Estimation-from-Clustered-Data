---
title: "Sim Poisson Beta-a1-b2-l20"
author: "Baoxing Liu"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r message=FALSE,warning=FALSE}
# ===============================================================
# Parallel Simulation: SRS vs Clustered Kernel Estimator
# Using Beta-Poisson Clustered Model (Windows-compatible)
# ===============================================================

library(tidyverse)
library(parallel)
library(scModels)

# ----------------------------
# 1. Discrete Triangular Kernel
# ----------------------------
discrete_triang_kernel <- function(xi, x, h, a) {
  num <- (a + 1)^h - abs(xi - x)^h
  num[num < 0] <- 0
  P_ah <- (2 * a + 1) * (a + 1)^h - 2 * sum((0:a)^h)
  return(num / P_ah)
}

discrete_triang_estimator <- function(xi, x, h, a) {
  mean(discrete_triang_kernel(xi, x, h, a))
}

# ----------------------------
# 2. Leave-One-Cluster-Out Estimator
# ----------------------------
loco_triang_estimator <- function(x, data, cluster_to_exclude, h, a) {
  train_data <- data[data$cluster != cluster_to_exclude, ]
  mean(discrete_triang_kernel(train_data$X, x, h, a))
}

# ----------------------------
# 3. Cross-Validation Objective
# ----------------------------
cv_objective <- function(h, data, a) {
  clusters <- unique(data$cluster)
  n <- nrow(data)
  sum1 <- 0
  sum2 <- 0
  
  for (c in clusters) {
    Cc_indices <- which(data$cluster == c)
    mc <- length(Cc_indices)
    
    for (i in Cc_indices) {
      sum1 <- sum1 + loco_triang_estimator(data$X[i], data, c, h, a)
    }
    
    x_points <- min(data$X):max(data$X)
    f_squared_sum <- sum(sapply(x_points, function(x) {
      loco_triang_estimator(x, data, c, h, a)^2
    }))
    
    sum2 <- sum2 + mc * f_squared_sum
  }
  
  cv_val <- -2/n * sum1 + 1/n * sum2
  return(cv_val)
}

# ===============================================================
# 4. Simulation Setup
# ===============================================================

set.seed(123)

lambda <- 20
alpha <- 1
beta <- 2

xs <- 0:35
k_grid <- 5
m <- 30
h_grid <- seq(0.1, 3.0, by = 0.2)
a <- 1
B <- 500
n_cores <- max(1, detectCores() - 1)

# ===============================================================
# 5. Main Simulation Loop (Windows parLapply)
# ===============================================================

mse_all <- list()
mise_all <- list()

for (k in k_grid) {
  
  # True pmf under Poisson–Beta
  f_true <- dpb(x = xs, alpha = alpha, beta = beta, c = lambda)
  
  # --------------------------
  # Create PSOCK cluster
  # --------------------------
  cl <- makeCluster(n_cores)
  
  # Export all needed variables + functions
  clusterExport(
    cl,
    varlist = c(
      "k", "m", "xs", "a", "lambda", "alpha", "beta", 
      "h_grid", "f_true",
      "discrete_triang_kernel",
      "discrete_triang_estimator",
      "loco_triang_estimator",
      "cv_objective"
    ),
    envir = environment()
  )
  
  # Load tidyverse on workers
  clusterEvalQ(cl, library(tidyverse))
  
  # --------------------------
  # Parallel Monte Carlo
  # --------------------------
  res_list <- parLapply(cl, 1:B, function(b) {
    
    # Simulate clustered Poisson–Beta model
    pc <- rbeta(k, alpha, beta)
    X <- unlist(lapply(pc, function(p) rpois(m, lambda * p)))
    
    df_clust <- data.frame(cluster = rep(1:k, each = m), X = X)
    df_srs   <- data.frame(cluster = 1:(k*m), X = X)
    
    # Bandwidth CV
    cv_scores_srs   <- sapply(h_grid, function(h) cv_objective(h, df_srs, a))
    cv_scores_clust <- sapply(h_grid, function(h) cv_objective(h, df_clust, a))
    
    best_h_srs   <- h_grid[which.min(cv_scores_srs)]
    best_h_clust <- h_grid[which.min(cv_scores_clust)]
    
    # PMF estimates
    fhat_srs_vals <- sapply(xs,
        function(x) discrete_triang_estimator(df_srs$X, x, best_h_srs, a))
    fhat_clust_vals <- sapply(xs,
        function(x) discrete_triang_estimator(df_clust$X, x, best_h_clust, a))
    
    # Errors
    se_srs   <- (fhat_srs_vals - f_true)^2
    se_clust <- (fhat_clust_vals - f_true)^2
    ise_srs    <- sum(se_srs)
    ise_clust  <- sum(se_clust)
    
    list(
      se_srs = se_srs,
      se_clust = se_clust,
      ise_srs = ise_srs,
      ise_clust = ise_clust
    )
  })
  
  stopCluster(cl)
  
  # --------------------------
  # Combine results
  # --------------------------
  se_srs_mat   <- do.call(cbind, lapply(res_list, `[[`, "se_srs"))
  se_clust_mat <- do.call(cbind, lapply(res_list, `[[`, "se_clust"))
  
  ise_srs   <- sapply(res_list, `[[`, "ise_srs")
  ise_clust <- sapply(res_list, `[[`, "ise_clust")
  
  mse_srs   <- rowMeans(se_srs_mat)
  mse_clust <- rowMeans(se_clust_mat)
  
  mse_df <- data.frame(
    x = xs,
    MSE_SRS = mse_srs,
    MSE_Clustered = mse_clust,
    k = k
  ) %>%
    pivot_longer(cols = c(MSE_SRS, MSE_Clustered),
                 names_to = "Method", values_to = "MSE")
  
  mse_all[[paste0("k", k)]] <- mse_df
  
  mise_tbl <- tibble(
    k = k,
    Method = c("SRS", "Clustered"),
    MISE = c(mean(ise_srs), mean(ise_clust)),
    SD_ISE = c(sd(ise_srs), sd(ise_clust))
  )
  
  mise_all[[paste0("k", k)]] <- mise_tbl
  
  cat("Finished k =", k, "\n")
}

# ===============================================================
# 6. Save results
# ===============================================================
mse_all_df <- bind_rows(mse_all)
mise_all_df <- bind_rows(mise_all)

write.csv(mse_all_df,  "mse_results_poisson_beta_a1_b2_l20.csv",  row.names = FALSE)
write.csv(mise_all_df, "mise_results_poisson_beta_a1.b2_l20.csv", row.names = FALSE)

# ===============================================================
# 7. MSE Plot
# ===============================================================
ggplot(mse_all_df, aes(x = x, y = sqrt(MSE), color = Method)) +
  geom_line() + geom_point(size = 0.5) +
  facet_grid(k ~., scales = "free_y") +
  theme_minimal(base_size = 13) +
  labs(
    title = "MSE Comparison: SRS vs Clustered Kernel Estimator",
    subtitle = "Poisson–Beta Cluster Model",
    x = "x", y = "sqrt(MSE)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "none"
  )

# ===============================================================
# 8. MISE Table
# ===============================================================
mise_all_df %>%
  arrange(k, Method) %>%
  knitr::kable(caption = "MISE Summary (Poisson–Beta Model)")
```

```{r message=FALSE,warning=FALSE}
# ===============================================================
# Parallel Simulation: SRS vs Clustered Kernel Estimator
# Using Beta-Poisson Clustered Model (Windows-compatible)
# ===============================================================

library(tidyverse)
library(parallel)
library(scModels)

# ----------------------------
# 1. Discrete Triangular Kernel
# ----------------------------
discrete_triang_kernel <- function(xi, x, h, a) {
  num <- (a + 1)^h - abs(xi - x)^h
  num[num < 0] <- 0
  P_ah <- (2 * a + 1) * (a + 1)^h - 2 * sum((0:a)^h)
  return(num / P_ah)
}

discrete_triang_estimator <- function(xi, x, h, a) {
  mean(discrete_triang_kernel(xi, x, h, a))
}

# ----------------------------
# 2. Leave-One-Cluster-Out Estimator
# ----------------------------
loco_triang_estimator <- function(x, data, cluster_to_exclude, h, a) {
  train_data <- data[data$cluster != cluster_to_exclude, ]
  mean(discrete_triang_kernel(train_data$X, x, h, a))
}

# ----------------------------
# 3. Cross-Validation Objective
# ----------------------------
cv_objective <- function(h, data, a) {
  clusters <- unique(data$cluster)
  n <- nrow(data)
  sum1 <- 0
  sum2 <- 0
  
  for (c in clusters) {
    Cc_indices <- which(data$cluster == c)
    mc <- length(Cc_indices)
    
    for (i in Cc_indices) {
      sum1 <- sum1 + loco_triang_estimator(data$X[i], data, c, h, a)
    }
    
    x_points <- min(data$X):max(data$X)
    f_squared_sum <- sum(sapply(x_points, function(x) {
      loco_triang_estimator(x, data, c, h, a)^2
    }))
    
    sum2 <- sum2 + mc * f_squared_sum
  }
  
  cv_val <- -2/n * sum1 + 1/n * sum2
  return(cv_val)
}

# ===============================================================
# 4. Simulation Setup
# ===============================================================

set.seed(123)

lambda <- 20
alpha <- 1
beta <- 2

xs <- 0:35
k_grid <- 5
m <- 60
h_grid <- seq(0.1, 3.0, by = 0.2)
a <- 1
B <- 500
n_cores <- max(1, detectCores() - 1)

# ===============================================================
# 5. Main Simulation Loop (Windows parLapply)
# ===============================================================

mse_all <- list()
mise_all <- list()

for (k in k_grid) {
  
  # True pmf under Poisson–Beta
  f_true <- dpb(x = xs, alpha = alpha, beta = beta, c = lambda)
  
  # --------------------------
  # Create PSOCK cluster
  # --------------------------
  cl <- makeCluster(n_cores)
  
  # Export all needed variables + functions
  clusterExport(
    cl,
    varlist = c(
      "k", "m", "xs", "a", "lambda", "alpha", "beta", 
      "h_grid", "f_true",
      "discrete_triang_kernel",
      "discrete_triang_estimator",
      "loco_triang_estimator",
      "cv_objective"
    ),
    envir = environment()
  )
  
  # Load tidyverse on workers
  clusterEvalQ(cl, library(tidyverse))
  
  # --------------------------
  # Parallel Monte Carlo
  # --------------------------
  res_list <- parLapply(cl, 1:B, function(b) {
    
    # Simulate clustered Poisson–Beta model
    pc <- rbeta(k, alpha, beta)
    X <- unlist(lapply(pc, function(p) rpois(m, lambda * p)))
    
    df_clust <- data.frame(cluster = rep(1:k, each = m), X = X)
    df_srs   <- data.frame(cluster = 1:(k*m), X = X)
    
    # Bandwidth CV
    cv_scores_srs   <- sapply(h_grid, function(h) cv_objective(h, df_srs, a))
    cv_scores_clust <- sapply(h_grid, function(h) cv_objective(h, df_clust, a))
    
    best_h_srs   <- h_grid[which.min(cv_scores_srs)]
    best_h_clust <- h_grid[which.min(cv_scores_clust)]
    
    # PMF estimates
    fhat_srs_vals <- sapply(xs,
        function(x) discrete_triang_estimator(df_srs$X, x, best_h_srs, a))
    fhat_clust_vals <- sapply(xs,
        function(x) discrete_triang_estimator(df_clust$X, x, best_h_clust, a))
    
    # Errors
    se_srs   <- (fhat_srs_vals - f_true)^2
    se_clust <- (fhat_clust_vals - f_true)^2
    ise_srs    <- sum(se_srs)
    ise_clust  <- sum(se_clust)
    
    list(
      se_srs = se_srs,
      se_clust = se_clust,
      ise_srs = ise_srs,
      ise_clust = ise_clust
    )
  })
  
  stopCluster(cl)
  
  # --------------------------
  # Combine results
  # --------------------------
  se_srs_mat   <- do.call(cbind, lapply(res_list, `[[`, "se_srs"))
  se_clust_mat <- do.call(cbind, lapply(res_list, `[[`, "se_clust"))
  
  ise_srs   <- sapply(res_list, `[[`, "ise_srs")
  ise_clust <- sapply(res_list, `[[`, "ise_clust")
  
  mse_srs   <- rowMeans(se_srs_mat)
  mse_clust <- rowMeans(se_clust_mat)
  
  mse_df <- data.frame(
    x = xs,
    MSE_SRS = mse_srs,
    MSE_Clustered = mse_clust,
    k = k
  ) %>%
    pivot_longer(cols = c(MSE_SRS, MSE_Clustered),
                 names_to = "Method", values_to = "MSE")
  
  mse_all[[paste0("k", k)]] <- mse_df
  
  mise_tbl <- tibble(
    k = k,
    Method = c("SRS", "Clustered"),
    MISE = c(mean(ise_srs), mean(ise_clust)),
    SD_ISE = c(sd(ise_srs), sd(ise_clust))
  )
  
  mise_all[[paste0("k", k)]] <- mise_tbl
  
  cat("Finished k =", k, "\n")
}

# ===============================================================
# 6. Save results
# ===============================================================
mse_all_df <- bind_rows(mse_all)
mise_all_df <- bind_rows(mise_all)

write.csv(mse_all_df,  "mse_results_poisson_beta_a1_b2_l20_k5.csv",  row.names = FALSE)
write.csv(mise_all_df, "mise_results_poisson_beta_a1.b2_l20_k5.csv", row.names = FALSE)

# ===============================================================
# 7. MSE Plot
# ===============================================================
ggplot(mse_all_df, aes(x = x, y = sqrt(MSE), color = Method)) +
  geom_line() + geom_point(size = 0.5) +
  facet_grid(k ~., scales = "free_y") +
  theme_minimal(base_size = 13) +
  labs(
    title = "MSE Comparison: SRS vs Clustered Kernel Estimator",
    subtitle = "Poisson–Beta Cluster Model",
    x = "x", y = "sqrt(MSE)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "none"
  )

# ===============================================================
# 8. MISE Table
# ===============================================================
mise_all_df %>%
  arrange(k, Method) %>%
  knitr::kable(caption = "MISE Summary (Poisson–Beta Model)")
```

```{r message=FALSE,warning=FALSE}
# ===============================================================
# Parallel Simulation: SRS vs Clustered Kernel Estimator
# Using Beta-Poisson Clustered Model (Windows-compatible)
# ===============================================================

library(tidyverse)
library(parallel)
library(scModels)

# ----------------------------
# 1. Discrete Triangular Kernel
# ----------------------------
discrete_triang_kernel <- function(xi, x, h, a) {
  num <- (a + 1)^h - abs(xi - x)^h
  num[num < 0] <- 0
  P_ah <- (2 * a + 1) * (a + 1)^h - 2 * sum((0:a)^h)
  return(num / P_ah)
}

discrete_triang_estimator <- function(xi, x, h, a) {
  mean(discrete_triang_kernel(xi, x, h, a))
}

# ----------------------------
# 2. Leave-One-Cluster-Out Estimator
# ----------------------------
loco_triang_estimator <- function(x, data, cluster_to_exclude, h, a) {
  train_data <- data[data$cluster != cluster_to_exclude, ]
  mean(discrete_triang_kernel(train_data$X, x, h, a))
}

# ----------------------------
# 3. Cross-Validation Objective
# ----------------------------
cv_objective <- function(h, data, a) {
  clusters <- unique(data$cluster)
  n <- nrow(data)
  sum1 <- 0
  sum2 <- 0
  
  for (c in clusters) {
    Cc_indices <- which(data$cluster == c)
    mc <- length(Cc_indices)
    
    for (i in Cc_indices) {
      sum1 <- sum1 + loco_triang_estimator(data$X[i], data, c, h, a)
    }
    
    x_points <- min(data$X):max(data$X)
    f_squared_sum <- sum(sapply(x_points, function(x) {
      loco_triang_estimator(x, data, c, h, a)^2
    }))
    
    sum2 <- sum2 + mc * f_squared_sum
  }
  
  cv_val <- -2/n * sum1 + 1/n * sum2
  return(cv_val)
}

# ===============================================================
# 4. Simulation Setup
# ===============================================================

set.seed(123)

lambda <- 20
alpha <- 1
beta <- 2

xs <- 0:35
k_grid <- 15
m <- 10
h_grid <- seq(0.1, 3.0, by = 0.2)
a <- 1
B <- 500
n_cores <- max(1, detectCores() - 1)

# ===============================================================
# 5. Main Simulation Loop (Windows parLapply)
# ===============================================================

mse_all <- list()
mise_all <- list()

for (k in k_grid) {
  
  # True pmf under Poisson–Beta
  f_true <- dpb(x = xs, alpha = alpha, beta = beta, c = lambda)
  
  # --------------------------
  # Create PSOCK cluster
  # --------------------------
  cl <- makeCluster(n_cores)
  
  # Export all needed variables + functions
  clusterExport(
    cl,
    varlist = c(
      "k", "m", "xs", "a", "lambda", "alpha", "beta", 
      "h_grid", "f_true",
      "discrete_triang_kernel",
      "discrete_triang_estimator",
      "loco_triang_estimator",
      "cv_objective"
    ),
    envir = environment()
  )
  
  # Load tidyverse on workers
  clusterEvalQ(cl, library(tidyverse))
  
  # --------------------------
  # Parallel Monte Carlo
  # --------------------------
  res_list <- parLapply(cl, 1:B, function(b) {
    
    # Simulate clustered Poisson–Beta model
    pc <- rbeta(k, alpha, beta)
    X <- unlist(lapply(pc, function(p) rpois(m, lambda * p)))
    
    df_clust <- data.frame(cluster = rep(1:k, each = m), X = X)
    df_srs   <- data.frame(cluster = 1:(k*m), X = X)
    
    # Bandwidth CV
    cv_scores_srs   <- sapply(h_grid, function(h) cv_objective(h, df_srs, a))
    cv_scores_clust <- sapply(h_grid, function(h) cv_objective(h, df_clust, a))
    
    best_h_srs   <- h_grid[which.min(cv_scores_srs)]
    best_h_clust <- h_grid[which.min(cv_scores_clust)]
    
    # PMF estimates
    fhat_srs_vals <- sapply(xs,
        function(x) discrete_triang_estimator(df_srs$X, x, best_h_srs, a))
    fhat_clust_vals <- sapply(xs,
        function(x) discrete_triang_estimator(df_clust$X, x, best_h_clust, a))
    
    # Errors
    se_srs   <- (fhat_srs_vals - f_true)^2
    se_clust <- (fhat_clust_vals - f_true)^2
    ise_srs    <- sum(se_srs)
    ise_clust  <- sum(se_clust)
    
    list(
      se_srs = se_srs,
      se_clust = se_clust,
      ise_srs = ise_srs,
      ise_clust = ise_clust
    )
  })
  
  stopCluster(cl)
  
  # --------------------------
  # Combine results
  # --------------------------
  se_srs_mat   <- do.call(cbind, lapply(res_list, `[[`, "se_srs"))
  se_clust_mat <- do.call(cbind, lapply(res_list, `[[`, "se_clust"))
  
  ise_srs   <- sapply(res_list, `[[`, "ise_srs")
  ise_clust <- sapply(res_list, `[[`, "ise_clust")
  
  mse_srs   <- rowMeans(se_srs_mat)
  mse_clust <- rowMeans(se_clust_mat)
  
  mse_df <- data.frame(
    x = xs,
    MSE_SRS = mse_srs,
    MSE_Clustered = mse_clust,
    k = k
  ) %>%
    pivot_longer(cols = c(MSE_SRS, MSE_Clustered),
                 names_to = "Method", values_to = "MSE")
  
  mse_all[[paste0("k", k)]] <- mse_df
  
  mise_tbl <- tibble(
    k = k,
    Method = c("SRS", "Clustered"),
    MISE = c(mean(ise_srs), mean(ise_clust)),
    SD_ISE = c(sd(ise_srs), sd(ise_clust))
  )
  
  mise_all[[paste0("k", k)]] <- mise_tbl
  
  cat("Finished k =", k, "\n")
}

# ===============================================================
# 6. Save results
# ===============================================================
mse_all_df <- bind_rows(mse_all)
mise_all_df <- bind_rows(mise_all)

write.csv(mse_all_df,  "mse_results_poisson_beta_a1_b2_l20_k15.csv",  row.names = FALSE)
write.csv(mise_all_df, "mise_results_poisson_beta_a1.b2_l20_k15.csv", row.names = FALSE)

# ===============================================================
# 7. MSE Plot
# ===============================================================
ggplot(mse_all_df, aes(x = x, y = sqrt(MSE), color = Method)) +
  geom_line() + geom_point(size = 0.5) +
  facet_grid(k ~., scales = "free_y") +
  theme_minimal(base_size = 13) +
  labs(
    title = "MSE Comparison: SRS vs Clustered Kernel Estimator",
    subtitle = "Poisson–Beta Cluster Model",
    x = "x", y = "sqrt(MSE)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "none"
  )

# ===============================================================
# 8. MISE Table
# ===============================================================
mise_all_df %>%
  arrange(k, Method) %>%
  knitr::kable(caption = "MISE Summary (Poisson–Beta Model)")
```

```{r message=FALSE,warning=FALSE}
# ===============================================================
# Parallel Simulation: SRS vs Clustered Kernel Estimator
# Using Beta-Poisson Clustered Model (Windows-compatible)
# ===============================================================

library(tidyverse)
library(parallel)
library(scModels)

# ----------------------------
# 1. Discrete Triangular Kernel
# ----------------------------
discrete_triang_kernel <- function(xi, x, h, a) {
  num <- (a + 1)^h - abs(xi - x)^h
  num[num < 0] <- 0
  P_ah <- (2 * a + 1) * (a + 1)^h - 2 * sum((0:a)^h)
  return(num / P_ah)
}

discrete_triang_estimator <- function(xi, x, h, a) {
  mean(discrete_triang_kernel(xi, x, h, a))
}

# ----------------------------
# 2. Leave-One-Cluster-Out Estimator
# ----------------------------
loco_triang_estimator <- function(x, data, cluster_to_exclude, h, a) {
  train_data <- data[data$cluster != cluster_to_exclude, ]
  mean(discrete_triang_kernel(train_data$X, x, h, a))
}

# ----------------------------
# 3. Cross-Validation Objective
# ----------------------------
cv_objective <- function(h, data, a) {
  clusters <- unique(data$cluster)
  n <- nrow(data)
  sum1 <- 0
  sum2 <- 0
  
  for (c in clusters) {
    Cc_indices <- which(data$cluster == c)
    mc <- length(Cc_indices)
    
    for (i in Cc_indices) {
      sum1 <- sum1 + loco_triang_estimator(data$X[i], data, c, h, a)
    }
    
    x_points <- min(data$X):max(data$X)
    f_squared_sum <- sum(sapply(x_points, function(x) {
      loco_triang_estimator(x, data, c, h, a)^2
    }))
    
    sum2 <- sum2 + mc * f_squared_sum
  }
  
  cv_val <- -2/n * sum1 + 1/n * sum2
  return(cv_val)
}

# ===============================================================
# 4. Simulation Setup
# ===============================================================

set.seed(123)

lambda <- 20
alpha <- 1
beta <- 2

xs <- 0:35
k_grid <- 30
m <- 10
h_grid <- seq(0.1, 3.0, by = 0.2)
a <- 1
B <- 500
n_cores <- max(1, detectCores() - 1)

# ===============================================================
# 5. Main Simulation Loop (Windows parLapply)
# ===============================================================

mse_all <- list()
mise_all <- list()

for (k in k_grid) {
  
  # True pmf under Poisson–Beta
  f_true <- dpb(x = xs, alpha = alpha, beta = beta, c = lambda)
  
  # --------------------------
  # Create PSOCK cluster
  # --------------------------
  cl <- makeCluster(n_cores)
  
  # Export all needed variables + functions
  clusterExport(
    cl,
    varlist = c(
      "k", "m", "xs", "a", "lambda", "alpha", "beta", 
      "h_grid", "f_true",
      "discrete_triang_kernel",
      "discrete_triang_estimator",
      "loco_triang_estimator",
      "cv_objective"
    ),
    envir = environment()
  )
  
  # Load tidyverse on workers
  clusterEvalQ(cl, library(tidyverse))
  
  # --------------------------
  # Parallel Monte Carlo
  # --------------------------
  res_list <- parLapply(cl, 1:B, function(b) {
    
    # Simulate clustered Poisson–Beta model
    pc <- rbeta(k, alpha, beta)
    X <- unlist(lapply(pc, function(p) rpois(m, lambda * p)))
    
    df_clust <- data.frame(cluster = rep(1:k, each = m), X = X)
    df_srs   <- data.frame(cluster = 1:(k*m), X = X)
    
    # Bandwidth CV
    cv_scores_srs   <- sapply(h_grid, function(h) cv_objective(h, df_srs, a))
    cv_scores_clust <- sapply(h_grid, function(h) cv_objective(h, df_clust, a))
    
    best_h_srs   <- h_grid[which.min(cv_scores_srs)]
    best_h_clust <- h_grid[which.min(cv_scores_clust)]
    
    # PMF estimates
    fhat_srs_vals <- sapply(xs,
        function(x) discrete_triang_estimator(df_srs$X, x, best_h_srs, a))
    fhat_clust_vals <- sapply(xs,
        function(x) discrete_triang_estimator(df_clust$X, x, best_h_clust, a))
    
    # Errors
    se_srs   <- (fhat_srs_vals - f_true)^2
    se_clust <- (fhat_clust_vals - f_true)^2
    ise_srs    <- sum(se_srs)
    ise_clust  <- sum(se_clust)
    
    list(
      se_srs = se_srs,
      se_clust = se_clust,
      ise_srs = ise_srs,
      ise_clust = ise_clust
    )
  })
  
  stopCluster(cl)
  
  # --------------------------
  # Combine results
  # --------------------------
  se_srs_mat   <- do.call(cbind, lapply(res_list, `[[`, "se_srs"))
  se_clust_mat <- do.call(cbind, lapply(res_list, `[[`, "se_clust"))
  
  ise_srs   <- sapply(res_list, `[[`, "ise_srs")
  ise_clust <- sapply(res_list, `[[`, "ise_clust")
  
  mse_srs   <- rowMeans(se_srs_mat)
  mse_clust <- rowMeans(se_clust_mat)
  
  mse_df <- data.frame(
    x = xs,
    MSE_SRS = mse_srs,
    MSE_Clustered = mse_clust,
    k = k
  ) %>%
    pivot_longer(cols = c(MSE_SRS, MSE_Clustered),
                 names_to = "Method", values_to = "MSE")
  
  mse_all[[paste0("k", k)]] <- mse_df
  
  mise_tbl <- tibble(
    k = k,
    Method = c("SRS", "Clustered"),
    MISE = c(mean(ise_srs), mean(ise_clust)),
    SD_ISE = c(sd(ise_srs), sd(ise_clust))
  )
  
  mise_all[[paste0("k", k)]] <- mise_tbl
  
  cat("Finished k =", k, "\n")
}

# ===============================================================
# 6. Save results
# ===============================================================
mse_all_df <- bind_rows(mse_all)
mise_all_df <- bind_rows(mise_all)

write.csv(mse_all_df,  "mse_results_poisson_beta_a1_b2_l20_k30.csv",  row.names = FALSE)
write.csv(mise_all_df, "mise_results_poisson_beta_a1.b2_l20_k30.csv", row.names = FALSE)

# ===============================================================
# 7. MSE Plot
# ===============================================================
ggplot(mse_all_df, aes(x = x, y = sqrt(MSE), color = Method)) +
  geom_line() + geom_point(size = 0.5) +
  facet_grid(k ~., scales = "free_y") +
  theme_minimal(base_size = 13) +
  labs(
    title = "MSE Comparison: SRS vs Clustered Kernel Estimator",
    subtitle = "Poisson–Beta Cluster Model",
    x = "x", y = "sqrt(MSE)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
    legend.position = "none"
  )

# ===============================================================
# 8. MISE Table
# ===============================================================
mise_all_df %>%
  arrange(k, Method) %>%
  knitr::kable(caption = "MISE Summary (Poisson–Beta Model)")
```

```{r}
library(tidyverse)
library(kableExtra)

mise1 = read_csv("mise_results_poisson_beta_a1.b2_l20.csv") %>% 
  filter(k == 5) %>% mutate(MISE = round(MISE, 5), SD_ISE = round(SD_ISE, 5))
mise = mise1$MISE
even_indices <- seq(from = 2, to = length(mise), by = 2)
odd_indices <- seq(from = 1, to = length(mise) - 1, by = 2)
mise[even_indices] <- round(mise[even_indices] / mise[odd_indices]*100,2)
mise[odd_indices] <- round(mise[odd_indices] / mise[odd_indices]*100,2)
df_mise = data.frame(Percent = mise)
mise1 = bind_cols(mise1, df_mise)

mise2 = read_csv("mise_results_poisson_beta_a1.b2_l20_k5.csv") %>% 
   mutate(MISE = round(MISE, 5), SD_ISE = round(SD_ISE, 5))
mise = mise2$MISE
even_indices <- seq(from = 2, to = length(mise), by = 2)
odd_indices <- seq(from = 1, to = length(mise) - 1, by = 2)
mise[even_indices] <- round(mise[even_indices] / mise[odd_indices]*100,2)
mise[odd_indices] <- round(mise[odd_indices] / mise[odd_indices]*100,2)
df_mise = data.frame(Percent = mise)
mise2 = bind_cols(mise2, df_mise)

mise3 = read_csv("mise_results_poisson_beta_a1.b2_l20_k15.csv") %>% 
   mutate(MISE = round(MISE, 5), SD_ISE = round(SD_ISE, 5))
mise = mise3$MISE
even_indices <- seq(from = 2, to = length(mise), by = 2)
odd_indices <- seq(from = 1, to = length(mise) - 1, by = 2)
mise[even_indices] <- round(mise[even_indices] / mise[odd_indices]*100,2)
mise[odd_indices] <- round(mise[odd_indices] / mise[odd_indices]*100,2)
df_mise = data.frame(Percent = mise)
mise3 = bind_cols(mise3, df_mise)

mise4 = read_csv("mise_results_poisson_beta_a1.b2_l20_k30.csv") %>% 
   mutate(MISE = round(MISE, 5), SD_ISE = round(SD_ISE, 5))
mise = mise4$MISE
even_indices <- seq(from = 2, to = length(mise), by = 2)
odd_indices <- seq(from = 1, to = length(mise) - 1, by = 2)
mise[even_indices] <- round(mise[even_indices] / mise[odd_indices]*100,2)
mise[odd_indices] <- round(mise[odd_indices] / mise[odd_indices]*100,2)
df_mise = data.frame(Percent = mise)
mise4 = bind_cols(mise4, df_mise)

df = bind_rows(mise1, mise2, mise3, mise4) %>% mutate(rho = rep(0.769, 8), 
      m = c(30, 30, 60, 60, 10, 10, 10, 10)) %>% 
  relocate(c(m, rho), .after = k)


table_df <- df %>%
      kable("latex", booktabs = TRUE, caption = "MISE for SRS and clustered descrete kernel estimators when rho = 0.769") %>%
      kable_styling(latex_options = c("striped", "hold_position"))
writeLines(table_df, "table_mise_rho_0.769.tex")
```

